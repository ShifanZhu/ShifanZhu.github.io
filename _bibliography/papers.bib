---
---
@article{zhu2021life,
  bibtex_show={true},
  preview={},
  abbr={IROS},
  abstract={Our paper proposes a direct sparse visual odometry method that combines event and RGBD data to estimate the pose of agile-legged robots during dynamic locomotion and acrobatic behaviors. Event cameras offer high temporal resolution and dynamic range, which can eliminate the issue of blurred RGB images during fast movements. This unique strength holds a potential for accurate pose estimation of agile- legged robots, which has been a challenging problem to tackle. Our framework leverages the benefits of both RGBD and event cameras to achieve robust and accurate pose estimation, even during dynamic maneuvers such as jumping and landing a quadruped robot, the Mini-Cheetah. Our major contributions are threefold: Firstly, we introduce an adaptive time surface (ATS) method that addresses the whiteout and blackout issue in conventional time surfaces by formulating pixel-wise decay rates based on scene complexity and motion speed. Secondly, we develop an effective pixel selection method that directly samples from event data and applies sample filtering through ATS, enabling us to pick pixels on distinct features. Lastly, we propose a nonlinear pose optimization formula that simultaneously performs 3D-2D alignment on both RGB-based and event-based maps and images, allowing the algorithm to fully exploit the benefits of both data streams. We extensively evaluate the performance of our framework on both the public dataset and our own quadruped robot dataset, demonstrating its effectiveness in accurately estimating the pose of agile robots during dynamic movements. Supplemental video: https://youtu.be/-5ieQShOg3M},
  title={Event camera-based visual odometry for dynamic motion tracking of a legged robot using adaptive time surface},
  author={Shifan Zhu, Xinyu Zhang, Shichun Guo, Jun Li, Huaping Liu},
  journal={IROS},
  year={2023},
  video={https://youtu.be/-5ieQShOg3M},
  html={https://ieeexplore.ieee.org/abstract/document/10342048},
  selected={true},
}


@article{zhu2023dynamic,
  bibtex_show={true},
  preview={iros-w.gif},
  abbr={IROS},
  abstract={As robots increase in agility and encounter fast- moving objects, dynamic object detection and avoidance become notably challenging. Traditional RGB cameras, burdened by motion blur and high latency, often act as the bottleneck. Event cameras have recently emerged as a promising solution for the challenges related to rapid movement. In this paper, we introduce a dynamic object avoidance framework that integrates both event and RGBD cameras. Specifically, this framework first estimates and compensates for the eventâ€™s motion to detect dynamic objects. Subsequently, depth data is combined to derive a 3D trajectory. When initiating from a static state, the robot adjusts its height based on the predicted collision point to avoid the dynamic obstacle. Through real-world experiments with the Mini-Cheetah, our approach successfully circumvents dynamic objects at speeds up to 5 m/s, achieving an 83% success rate.
Supplemental video: },
  title={Dynamic Object Avoidance using Event-Data for a Quadruped Robot},
  author={Zhu, Shifan and Perera, Nisal and Yu, Shangqun and Hwang, Hochul and Kim, Donghyun},
  journal={IROS IPPC Workshop},
  year={2023},
  video={https://www.youtube.com/watch?v=wEPvynkVlLA},
  html={https://ippc-iros23.github.io/papers/zhu.pdf},
  selected={true},
}

@article{zhu2021life,
  bibtex_show={true},
  preview={},
  abbr={ICRA},
  abstract={Mapping and localization in non-static environments are fundamental problems in robotics. Most of previous methods mainly focus on static and highly dynamic objects in the environment, which may suffer from localization failure in semi-dynamic scenarios without considering objects with lower dynamics, such as parked cars and stopped pedestrians. In this paper, we introduce semantic mapping and lifelong localization approaches to recognize semi-dynamic objects in non-static environments. We also propose a generic framework that can integrate mainstream object detection algorithms with mapping and localization algorithms. The mapping method combines an object detection algorithm and a SLAM algorithm to detect semi-dynamic objects and constructs a semantic map that only contains semi-dynamic objects in the environment. During navigation, the localization method can classify observation corresponding to static and non-static objects respectively and evaluate whether those semi-dynamic objects have moved, to reduce the weight of invalid observation and localization fluctuation. Real-world experiments show that the proposed method can improve the localization accuracy of mobile robots in non-static scenarios.},
  author={Shifan Zhu, Xinyu Zhang, Shichun Guo, Jun Li, Huaping Liu},
  journal={ICRA},
  year={2021},
  video={},
  html={https://ieeexplore.ieee.org/abstract/document/9561584},
  selected={true},
}

@article{zhang2021line,
  bibtex_show={true},
  preview={},
  abbr={ICRA},
  abstract={Reliable real-time extrinsic parameters of 3D Light Detection and Ranging (LiDAR) and camera are a key component of multi-modal perception systems. However, extrinsic transformation may drift gradually during operation, which can result in decreased accuracy of perception system. To solve this problem, we propose a line-based method that enables automatic online extrinsic calibration of LiDAR and camera in real-world scenes. Herein, the line feature is selected to constrain the extrinsic parameters for its ubiquity. Initially, the line features are extracted and filtered from point clouds and images. Afterwards, an adaptive optimization is utilized to provide accurate extrinsic parameters. We demonstrate that line features are robust geometric features that can be extracted from point clouds and images, thus contributing to the extrinsic calibration. To demonstrate the benefits of this method, we evaluate it on KITTI benchmark with ground truth value. The experiments verify the accuracy of the calibration approach. In online experiments on hundreds of frames, our approach automatically corrects miscalibration errors and achieves an accuracy of 0.2 degrees, which verifies its applicability in various scenarios. This work can provide basis for perception systems and further improve the performance of other algorithms that utilize these sensors.},
  title={Line-based Automatic Extrinsic Calibration of LiDAR and Camera},
  author={Zhang, Xinyu and Zhu, Shifan and Guo, Shichun and Li, Jun and Liu, Huaping},
  journal={ICRA},
  year={2021},
  video={},
  html={https://ieeexplore.ieee.org/abstract/document/9561216},
  selected={true},
}